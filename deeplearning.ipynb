{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA load\n",
    "X = pd.read_csv('./content/loan_train_preprocessed.csv')\n",
    "\n",
    "# backward\n",
    "X = X[['term', 'initial_list_status', 'int_rate', \n",
    "'emp_length', 'annual_inc', 'dti', 'delinq_2yrs', \n",
    "'inq_last_6mths', 'revol_util', 'recoveries', \n",
    "'collection_recovery_fee', 'tot_cur_bal', \n",
    "'home_ownershipRENT', 'purposesmall_business', \n",
    "'purposewedding', 'earliest_cr_line2000']]\n",
    "\n",
    "y = pd.read_csv('./content/loan_train_label.csv')\n",
    "y = y.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10240, 16), (10240, 1), (2560, 16), (2560, 1), (3200, 16), (3200, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividing the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.20 )\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size = 0.20 )\n",
    "\n",
    "x_train.shape,y_train.shape,x_val.shape,y_val.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2048, input_shape = (len(x_train.columns),), activation='ELU'))\n",
    "model.add(Dense(1024, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='ELU'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='ELU'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 2s 212ms/step - loss: 0.6897 - accuracy: 0.5869 - f1: 0.5884 - val_loss: 0.5990 - val_accuracy: 0.6652 - val_f1: 0.6289\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6146 - accuracy: 0.6465 - f1: 0.6187 - val_loss: 0.5853 - val_accuracy: 0.6727 - val_f1: 0.6494\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5968 - accuracy: 0.6578 - f1: 0.6409 - val_loss: 0.5722 - val_accuracy: 0.6711 - val_f1: 0.6193\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5907 - accuracy: 0.6449 - f1: 0.6096 - val_loss: 0.5709 - val_accuracy: 0.6586 - val_f1: 0.6899\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.5800 - accuracy: 0.6657 - f1: 0.6461 - val_loss: 0.5630 - val_accuracy: 0.6859 - val_f1: 0.6649\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 1s 193ms/step - loss: 0.5746 - accuracy: 0.6609 - f1: 0.6323 - val_loss: 0.5631 - val_accuracy: 0.6789 - val_f1: 0.6920\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5714 - accuracy: 0.6626 - f1: 0.6349 - val_loss: 0.5585 - val_accuracy: 0.6770 - val_f1: 0.6922\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5642 - accuracy: 0.6714 - f1: 0.6660 - val_loss: 0.5577 - val_accuracy: 0.6863 - val_f1: 0.6930\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.5675 - accuracy: 0.6630 - f1: 0.6328 - val_loss: 0.5574 - val_accuracy: 0.6773 - val_f1: 0.6943\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5686 - accuracy: 0.6647 - f1: 0.6499 - val_loss: 0.5606 - val_accuracy: 0.6785 - val_f1: 0.6898\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5697 - accuracy: 0.6658 - f1: 0.6374 - val_loss: 0.5569 - val_accuracy: 0.6832 - val_f1: 0.6863\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5658 - accuracy: 0.6671 - f1: 0.6451 - val_loss: 0.5574 - val_accuracy: 0.6832 - val_f1: 0.6692\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.5645 - accuracy: 0.6691 - f1: 0.6517 - val_loss: 0.5575 - val_accuracy: 0.6824 - val_f1: 0.6752\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5615 - accuracy: 0.6662 - f1: 0.6281 - val_loss: 0.5573 - val_accuracy: 0.6805 - val_f1: 0.6893\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5597 - accuracy: 0.6700 - f1: 0.6641 - val_loss: 0.5581 - val_accuracy: 0.6801 - val_f1: 0.6677\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.5610 - accuracy: 0.6721 - f1: 0.6324 - val_loss: 0.5548 - val_accuracy: 0.6832 - val_f1: 0.6948\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 0.5671 - accuracy: 0.6677 - f1: 0.6479 - val_loss: 0.5629 - val_accuracy: 0.6777 - val_f1: 0.6868\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5656 - accuracy: 0.6675 - f1: 0.6550 - val_loss: 0.5601 - val_accuracy: 0.6801 - val_f1: 0.6909\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5650 - accuracy: 0.6693 - f1: 0.6510 - val_loss: 0.5560 - val_accuracy: 0.6840 - val_f1: 0.6785\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.5595 - accuracy: 0.6742 - f1: 0.6508 - val_loss: 0.5532 - val_accuracy: 0.6879 - val_f1: 0.6931\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5583 - accuracy: 0.6734 - f1: 0.6548 - val_loss: 0.5573 - val_accuracy: 0.6781 - val_f1: 0.6834\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5589 - accuracy: 0.6712 - f1: 0.6577 - val_loss: 0.5558 - val_accuracy: 0.6824 - val_f1: 0.6809\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5571 - accuracy: 0.6726 - f1: 0.6547 - val_loss: 0.5521 - val_accuracy: 0.6867 - val_f1: 0.6788\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5595 - accuracy: 0.6759 - f1: 0.6393 - val_loss: 0.5550 - val_accuracy: 0.6828 - val_f1: 0.6845\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5586 - accuracy: 0.6733 - f1: 0.6591 - val_loss: 0.5539 - val_accuracy: 0.6879 - val_f1: 0.6930\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5584 - accuracy: 0.6715 - f1: 0.6508 - val_loss: 0.5513 - val_accuracy: 0.6867 - val_f1: 0.6901\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5560 - accuracy: 0.6774 - f1: 0.6559 - val_loss: 0.5543 - val_accuracy: 0.6848 - val_f1: 0.6918\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5554 - accuracy: 0.6786 - f1: 0.6641 - val_loss: 0.5552 - val_accuracy: 0.6773 - val_f1: 0.6897\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5549 - accuracy: 0.6821 - f1: 0.6679 - val_loss: 0.5527 - val_accuracy: 0.6844 - val_f1: 0.6856\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5560 - accuracy: 0.6746 - f1: 0.6471 - val_loss: 0.5523 - val_accuracy: 0.6812 - val_f1: 0.6829\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5552 - accuracy: 0.6760 - f1: 0.6567 - val_loss: 0.5523 - val_accuracy: 0.6844 - val_f1: 0.6982\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5528 - accuracy: 0.6793 - f1: 0.6615 - val_loss: 0.5506 - val_accuracy: 0.6867 - val_f1: 0.6895\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5618 - accuracy: 0.6691 - f1: 0.6491 - val_loss: 0.5614 - val_accuracy: 0.6750 - val_f1: 0.7034\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5665 - accuracy: 0.6679 - f1: 0.6532 - val_loss: 0.5611 - val_accuracy: 0.6746 - val_f1: 0.6712\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5602 - accuracy: 0.6762 - f1: 0.6713 - val_loss: 0.5562 - val_accuracy: 0.6816 - val_f1: 0.6845\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.5592 - accuracy: 0.6738 - f1: 0.6430 - val_loss: 0.5535 - val_accuracy: 0.6824 - val_f1: 0.7018\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5591 - accuracy: 0.6794 - f1: 0.6708 - val_loss: 0.5567 - val_accuracy: 0.6914 - val_f1: 0.6879\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5561 - accuracy: 0.6747 - f1: 0.6559 - val_loss: 0.5512 - val_accuracy: 0.6863 - val_f1: 0.6987\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5586 - accuracy: 0.6742 - f1: 0.6426 - val_loss: 0.5549 - val_accuracy: 0.6809 - val_f1: 0.6969\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5586 - accuracy: 0.6719 - f1: 0.6684 - val_loss: 0.5575 - val_accuracy: 0.6832 - val_f1: 0.6689\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5553 - accuracy: 0.6765 - f1: 0.6511 - val_loss: 0.5531 - val_accuracy: 0.6852 - val_f1: 0.6814\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5560 - accuracy: 0.6773 - f1: 0.6668 - val_loss: 0.5543 - val_accuracy: 0.6848 - val_f1: 0.6783\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.5566 - accuracy: 0.6751 - f1: 0.6475 - val_loss: 0.5552 - val_accuracy: 0.6773 - val_f1: 0.7013\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.5571 - accuracy: 0.6736 - f1: 0.6615 - val_loss: 0.5585 - val_accuracy: 0.6828 - val_f1: 0.6752\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.5546 - accuracy: 0.6794 - f1: 0.6630 - val_loss: 0.5542 - val_accuracy: 0.6859 - val_f1: 0.6732\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5551 - accuracy: 0.6759 - f1: 0.6513 - val_loss: 0.5542 - val_accuracy: 0.6922 - val_f1: 0.6897\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5530 - accuracy: 0.6795 - f1: 0.6588 - val_loss: 0.5540 - val_accuracy: 0.6883 - val_f1: 0.6858\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5517 - accuracy: 0.6809 - f1: 0.6639 - val_loss: 0.5522 - val_accuracy: 0.6902 - val_f1: 0.6877\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.5515 - accuracy: 0.6822 - f1: 0.6609 - val_loss: 0.5521 - val_accuracy: 0.6844 - val_f1: 0.6778\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.5549 - accuracy: 0.6792 - f1: 0.6684 - val_loss: 0.5542 - val_accuracy: 0.6797 - val_f1: 0.6678\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5491 - accuracy: 0.6770 - f1: 0.6456 - val_loss: 0.5544 - val_accuracy: 0.6836 - val_f1: 0.6991\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5532 - accuracy: 0.6768 - f1: 0.6629 - val_loss: 0.5534 - val_accuracy: 0.6832 - val_f1: 0.6716\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.5505 - accuracy: 0.6808 - f1: 0.6611 - val_loss: 0.5565 - val_accuracy: 0.6844 - val_f1: 0.6944\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.5546 - accuracy: 0.6729 - f1: 0.6541 - val_loss: 0.5525 - val_accuracy: 0.6836 - val_f1: 0.6857\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5507 - accuracy: 0.6835 - f1: 0.6781 - val_loss: 0.5562 - val_accuracy: 0.6785 - val_f1: 0.6626\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5538 - accuracy: 0.6763 - f1: 0.6577 - val_loss: 0.5543 - val_accuracy: 0.6820 - val_f1: 0.6849\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.5510 - accuracy: 0.6815 - f1: 0.6502 - val_loss: 0.5583 - val_accuracy: 0.6770 - val_f1: 0.6885\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.5501 - accuracy: 0.6828 - f1: 0.6742 - val_loss: 0.5587 - val_accuracy: 0.6812 - val_f1: 0.6766\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5482 - accuracy: 0.6769 - f1: 0.6560 - val_loss: 0.5532 - val_accuracy: 0.6785 - val_f1: 0.6851\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5495 - accuracy: 0.6784 - f1: 0.6571 - val_loss: 0.5550 - val_accuracy: 0.6793 - val_f1: 0.6895\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5514 - accuracy: 0.6800 - f1: 0.6704 - val_loss: 0.5584 - val_accuracy: 0.6852 - val_f1: 0.6683\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5523 - accuracy: 0.6818 - f1: 0.6648 - val_loss: 0.5543 - val_accuracy: 0.6797 - val_f1: 0.6900\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5532 - accuracy: 0.6793 - f1: 0.6516 - val_loss: 0.5547 - val_accuracy: 0.6781 - val_f1: 0.6897\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5505 - accuracy: 0.6771 - f1: 0.6592 - val_loss: 0.5546 - val_accuracy: 0.6871 - val_f1: 0.6839\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.5519 - accuracy: 0.6795 - f1: 0.6584 - val_loss: 0.5531 - val_accuracy: 0.6875 - val_f1: 0.6886\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5507 - accuracy: 0.6775 - f1: 0.6616 - val_loss: 0.5569 - val_accuracy: 0.6820 - val_f1: 0.6828\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5493 - accuracy: 0.6789 - f1: 0.6634 - val_loss: 0.5512 - val_accuracy: 0.6816 - val_f1: 0.6931\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5471 - accuracy: 0.6801 - f1: 0.6607 - val_loss: 0.5500 - val_accuracy: 0.6855 - val_f1: 0.6886\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5506 - accuracy: 0.6849 - f1: 0.6647 - val_loss: 0.5541 - val_accuracy: 0.6832 - val_f1: 0.6879\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5476 - accuracy: 0.6852 - f1: 0.6778 - val_loss: 0.5516 - val_accuracy: 0.6855 - val_f1: 0.6825\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5478 - accuracy: 0.6861 - f1: 0.6648 - val_loss: 0.5513 - val_accuracy: 0.6844 - val_f1: 0.6894\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5472 - accuracy: 0.6852 - f1: 0.6685 - val_loss: 0.5545 - val_accuracy: 0.6891 - val_f1: 0.6849\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5556 - accuracy: 0.6737 - f1: 0.6502 - val_loss: 0.5615 - val_accuracy: 0.6742 - val_f1: 0.6909\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5558 - accuracy: 0.6782 - f1: 0.6641 - val_loss: 0.5592 - val_accuracy: 0.6801 - val_f1: 0.6995\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5569 - accuracy: 0.6780 - f1: 0.6580 - val_loss: 0.5599 - val_accuracy: 0.6777 - val_f1: 0.6824\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5560 - accuracy: 0.6818 - f1: 0.6753 - val_loss: 0.5640 - val_accuracy: 0.6797 - val_f1: 0.6404\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5548 - accuracy: 0.6739 - f1: 0.6417 - val_loss: 0.5548 - val_accuracy: 0.6789 - val_f1: 0.6991\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5492 - accuracy: 0.6871 - f1: 0.6658 - val_loss: 0.5535 - val_accuracy: 0.6883 - val_f1: 0.6857\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5478 - accuracy: 0.6820 - f1: 0.6655 - val_loss: 0.5534 - val_accuracy: 0.6824 - val_f1: 0.6872\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5492 - accuracy: 0.6789 - f1: 0.6671 - val_loss: 0.5552 - val_accuracy: 0.6820 - val_f1: 0.6656\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5486 - accuracy: 0.6853 - f1: 0.6684 - val_loss: 0.5550 - val_accuracy: 0.6824 - val_f1: 0.6959\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5503 - accuracy: 0.6828 - f1: 0.6711 - val_loss: 0.5566 - val_accuracy: 0.6789 - val_f1: 0.6832\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5495 - accuracy: 0.6860 - f1: 0.6695 - val_loss: 0.5560 - val_accuracy: 0.6781 - val_f1: 0.6821\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5490 - accuracy: 0.6890 - f1: 0.6806 - val_loss: 0.5559 - val_accuracy: 0.6836 - val_f1: 0.6866\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5481 - accuracy: 0.6848 - f1: 0.6680 - val_loss: 0.5536 - val_accuracy: 0.6848 - val_f1: 0.6935\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5454 - accuracy: 0.6868 - f1: 0.6676 - val_loss: 0.5544 - val_accuracy: 0.6895 - val_f1: 0.6897\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5460 - accuracy: 0.6883 - f1: 0.6733 - val_loss: 0.5573 - val_accuracy: 0.6816 - val_f1: 0.6411\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5464 - accuracy: 0.6832 - f1: 0.6604 - val_loss: 0.5530 - val_accuracy: 0.6863 - val_f1: 0.6937\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5468 - accuracy: 0.6837 - f1: 0.6652 - val_loss: 0.5557 - val_accuracy: 0.6816 - val_f1: 0.6625\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.5471 - accuracy: 0.6821 - f1: 0.6658 - val_loss: 0.5576 - val_accuracy: 0.6809 - val_f1: 0.6982\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5468 - accuracy: 0.6869 - f1: 0.6712 - val_loss: 0.5538 - val_accuracy: 0.6906 - val_f1: 0.6845\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5485 - accuracy: 0.6847 - f1: 0.6632 - val_loss: 0.5548 - val_accuracy: 0.6836 - val_f1: 0.6603\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5474 - accuracy: 0.6786 - f1: 0.6623 - val_loss: 0.5580 - val_accuracy: 0.6785 - val_f1: 0.6967\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5487 - accuracy: 0.6789 - f1: 0.6614 - val_loss: 0.5574 - val_accuracy: 0.6871 - val_f1: 0.6850\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.5448 - accuracy: 0.6874 - f1: 0.6712 - val_loss: 0.5560 - val_accuracy: 0.6859 - val_f1: 0.6887\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5476 - accuracy: 0.6828 - f1: 0.6629 - val_loss: 0.5552 - val_accuracy: 0.6781 - val_f1: 0.6924\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5432 - accuracy: 0.6829 - f1: 0.6750 - val_loss: 0.5618 - val_accuracy: 0.6844 - val_f1: 0.6618\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5448 - accuracy: 0.6862 - f1: 0.6731 - val_loss: 0.5549 - val_accuracy: 0.6848 - val_f1: 0.6994\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5440 - accuracy: 0.6847 - f1: 0.6629 - val_loss: 0.5548 - val_accuracy: 0.6883 - val_f1: 0.6814\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5444 - accuracy: 0.6848 - f1: 0.6696 - val_loss: 0.5551 - val_accuracy: 0.6812 - val_f1: 0.6905\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5437 - accuracy: 0.6873 - f1: 0.6743 - val_loss: 0.5610 - val_accuracy: 0.6930 - val_f1: 0.6772\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5444 - accuracy: 0.6886 - f1: 0.6716 - val_loss: 0.5579 - val_accuracy: 0.6777 - val_f1: 0.6985\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.5467 - accuracy: 0.6826 - f1: 0.6732 - val_loss: 0.5572 - val_accuracy: 0.6805 - val_f1: 0.6813\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5492 - accuracy: 0.6838 - f1: 0.6695 - val_loss: 0.5598 - val_accuracy: 0.6770 - val_f1: 0.6695\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5473 - accuracy: 0.6813 - f1: 0.6635 - val_loss: 0.5606 - val_accuracy: 0.6793 - val_f1: 0.6883\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5471 - accuracy: 0.6794 - f1: 0.6584 - val_loss: 0.5571 - val_accuracy: 0.6855 - val_f1: 0.6814\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5457 - accuracy: 0.6828 - f1: 0.6629 - val_loss: 0.5581 - val_accuracy: 0.6812 - val_f1: 0.6870\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5454 - accuracy: 0.6851 - f1: 0.6701 - val_loss: 0.5570 - val_accuracy: 0.6855 - val_f1: 0.6648\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5392 - accuracy: 0.6918 - f1: 0.6750 - val_loss: 0.5549 - val_accuracy: 0.6898 - val_f1: 0.6985\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5411 - accuracy: 0.6889 - f1: 0.6780 - val_loss: 0.5576 - val_accuracy: 0.6773 - val_f1: 0.7025\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5397 - accuracy: 0.6892 - f1: 0.6745 - val_loss: 0.5546 - val_accuracy: 0.6863 - val_f1: 0.6996\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5417 - accuracy: 0.6873 - f1: 0.6777 - val_loss: 0.5569 - val_accuracy: 0.6863 - val_f1: 0.6907\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.5446 - accuracy: 0.6890 - f1: 0.6672 - val_loss: 0.5597 - val_accuracy: 0.6746 - val_f1: 0.6864\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5504 - accuracy: 0.6816 - f1: 0.6793 - val_loss: 0.5574 - val_accuracy: 0.6777 - val_f1: 0.6942\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5442 - accuracy: 0.6886 - f1: 0.6779 - val_loss: 0.5617 - val_accuracy: 0.6836 - val_f1: 0.7040\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.5420 - accuracy: 0.6905 - f1: 0.6876 - val_loss: 0.5598 - val_accuracy: 0.6902 - val_f1: 0.6751\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5443 - accuracy: 0.6839 - f1: 0.6579 - val_loss: 0.5572 - val_accuracy: 0.6754 - val_f1: 0.7015\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 0.5452 - accuracy: 0.6885 - f1: 0.6735 - val_loss: 0.5622 - val_accuracy: 0.6855 - val_f1: 0.6747\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5455 - accuracy: 0.6842 - f1: 0.6737 - val_loss: 0.5554 - val_accuracy: 0.6824 - val_f1: 0.6872\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5418 - accuracy: 0.6893 - f1: 0.6651 - val_loss: 0.5557 - val_accuracy: 0.6809 - val_f1: 0.6962\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5406 - accuracy: 0.6879 - f1: 0.6736 - val_loss: 0.5610 - val_accuracy: 0.6836 - val_f1: 0.6858\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5435 - accuracy: 0.6831 - f1: 0.6822 - val_loss: 0.5572 - val_accuracy: 0.6852 - val_f1: 0.6929\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.5380 - accuracy: 0.6940 - f1: 0.6728 - val_loss: 0.5585 - val_accuracy: 0.6809 - val_f1: 0.7063\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5413 - accuracy: 0.6872 - f1: 0.6714 - val_loss: 0.5587 - val_accuracy: 0.6902 - val_f1: 0.6720\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5423 - accuracy: 0.6860 - f1: 0.6822 - val_loss: 0.5594 - val_accuracy: 0.6785 - val_f1: 0.7013\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5401 - accuracy: 0.6913 - f1: 0.6774 - val_loss: 0.5582 - val_accuracy: 0.6844 - val_f1: 0.6823\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.5381 - accuracy: 0.6928 - f1: 0.6872 - val_loss: 0.5563 - val_accuracy: 0.6805 - val_f1: 0.6996\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.5355 - accuracy: 0.6955 - f1: 0.6806 - val_loss: 0.5587 - val_accuracy: 0.6902 - val_f1: 0.6801\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.5394 - accuracy: 0.6914 - f1: 0.6814 - val_loss: 0.5553 - val_accuracy: 0.6871 - val_f1: 0.6810\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.5400 - accuracy: 0.6905 - f1: 0.6741 - val_loss: 0.5653 - val_accuracy: 0.6672 - val_f1: 0.7022\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5407 - accuracy: 0.6914 - f1: 0.6910 - val_loss: 0.5619 - val_accuracy: 0.6820 - val_f1: 0.6839\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 1s 190ms/step - loss: 0.5390 - accuracy: 0.6901 - f1: 0.6878 - val_loss: 0.5603 - val_accuracy: 0.6883 - val_f1: 0.6720\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5386 - accuracy: 0.6899 - f1: 0.6645 - val_loss: 0.5568 - val_accuracy: 0.6805 - val_f1: 0.6963\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5387 - accuracy: 0.6942 - f1: 0.6815 - val_loss: 0.5574 - val_accuracy: 0.6852 - val_f1: 0.6900\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5388 - accuracy: 0.6937 - f1: 0.6879 - val_loss: 0.5588 - val_accuracy: 0.6840 - val_f1: 0.6682\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5454 - accuracy: 0.6873 - f1: 0.6564 - val_loss: 0.5566 - val_accuracy: 0.6871 - val_f1: 0.6961\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5383 - accuracy: 0.6953 - f1: 0.6908 - val_loss: 0.5602 - val_accuracy: 0.6824 - val_f1: 0.7024\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5382 - accuracy: 0.6933 - f1: 0.6833 - val_loss: 0.5582 - val_accuracy: 0.6891 - val_f1: 0.6818\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5340 - accuracy: 0.6953 - f1: 0.6865 - val_loss: 0.5657 - val_accuracy: 0.6852 - val_f1: 0.6678\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5370 - accuracy: 0.6944 - f1: 0.6838 - val_loss: 0.5576 - val_accuracy: 0.6902 - val_f1: 0.6943\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5334 - accuracy: 0.6936 - f1: 0.6684 - val_loss: 0.5650 - val_accuracy: 0.6766 - val_f1: 0.7061\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5355 - accuracy: 0.6981 - f1: 0.6998 - val_loss: 0.5601 - val_accuracy: 0.6891 - val_f1: 0.6737\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.5382 - accuracy: 0.6919 - f1: 0.6793 - val_loss: 0.5592 - val_accuracy: 0.6891 - val_f1: 0.6965\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5322 - accuracy: 0.6957 - f1: 0.6782 - val_loss: 0.5602 - val_accuracy: 0.6746 - val_f1: 0.7058\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5359 - accuracy: 0.6938 - f1: 0.6816 - val_loss: 0.5612 - val_accuracy: 0.6879 - val_f1: 0.6796\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5336 - accuracy: 0.6968 - f1: 0.6930 - val_loss: 0.5615 - val_accuracy: 0.6758 - val_f1: 0.7022\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5322 - accuracy: 0.6967 - f1: 0.6787 - val_loss: 0.5556 - val_accuracy: 0.6887 - val_f1: 0.6854\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.5324 - accuracy: 0.6938 - f1: 0.6831 - val_loss: 0.5602 - val_accuracy: 0.6836 - val_f1: 0.6424\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5344 - accuracy: 0.6934 - f1: 0.6700 - val_loss: 0.5708 - val_accuracy: 0.6676 - val_f1: 0.7036\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5365 - accuracy: 0.6947 - f1: 0.6975 - val_loss: 0.5554 - val_accuracy: 0.6883 - val_f1: 0.6864\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5353 - accuracy: 0.6966 - f1: 0.6726 - val_loss: 0.5557 - val_accuracy: 0.6938 - val_f1: 0.7031\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5363 - accuracy: 0.6969 - f1: 0.6938 - val_loss: 0.5573 - val_accuracy: 0.6887 - val_f1: 0.6842\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5327 - accuracy: 0.6979 - f1: 0.6832 - val_loss: 0.5622 - val_accuracy: 0.6875 - val_f1: 0.6980\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5331 - accuracy: 0.6958 - f1: 0.6855 - val_loss: 0.5586 - val_accuracy: 0.6824 - val_f1: 0.7027\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5340 - accuracy: 0.7009 - f1: 0.6930 - val_loss: 0.5649 - val_accuracy: 0.6820 - val_f1: 0.6972\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5479 - accuracy: 0.6883 - f1: 0.6664 - val_loss: 0.5733 - val_accuracy: 0.6695 - val_f1: 0.6998\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5603 - accuracy: 0.6765 - f1: 0.6561 - val_loss: 0.5712 - val_accuracy: 0.6805 - val_f1: 0.6604\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5565 - accuracy: 0.6824 - f1: 0.6853 - val_loss: 0.5782 - val_accuracy: 0.6824 - val_f1: 0.6918\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 0.5508 - accuracy: 0.6847 - f1: 0.6803 - val_loss: 0.5673 - val_accuracy: 0.6820 - val_f1: 0.6842\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5483 - accuracy: 0.6886 - f1: 0.6729 - val_loss: 0.5695 - val_accuracy: 0.6816 - val_f1: 0.6978\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5459 - accuracy: 0.6903 - f1: 0.6866 - val_loss: 0.5694 - val_accuracy: 0.6840 - val_f1: 0.6961\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5439 - accuracy: 0.6885 - f1: 0.6717 - val_loss: 0.5686 - val_accuracy: 0.6840 - val_f1: 0.6922\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5417 - accuracy: 0.6910 - f1: 0.6855 - val_loss: 0.5656 - val_accuracy: 0.6816 - val_f1: 0.6953\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5397 - accuracy: 0.6930 - f1: 0.6845 - val_loss: 0.5690 - val_accuracy: 0.6844 - val_f1: 0.6683\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.5430 - accuracy: 0.6915 - f1: 0.6759 - val_loss: 0.5647 - val_accuracy: 0.6883 - val_f1: 0.6940\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.5402 - accuracy: 0.6904 - f1: 0.6924 - val_loss: 0.5674 - val_accuracy: 0.6879 - val_f1: 0.6828\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5404 - accuracy: 0.6931 - f1: 0.6837 - val_loss: 0.5627 - val_accuracy: 0.6754 - val_f1: 0.6986\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.5395 - accuracy: 0.6951 - f1: 0.6851 - val_loss: 0.5652 - val_accuracy: 0.6840 - val_f1: 0.6920\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5368 - accuracy: 0.6952 - f1: 0.6946 - val_loss: 0.5673 - val_accuracy: 0.6855 - val_f1: 0.6692\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5373 - accuracy: 0.6940 - f1: 0.6841 - val_loss: 0.5703 - val_accuracy: 0.6754 - val_f1: 0.7021\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5337 - accuracy: 0.6960 - f1: 0.6919 - val_loss: 0.5663 - val_accuracy: 0.6840 - val_f1: 0.6614\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5299 - accuracy: 0.6996 - f1: 0.6836 - val_loss: 0.5660 - val_accuracy: 0.6848 - val_f1: 0.6916\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5314 - accuracy: 0.7002 - f1: 0.6945 - val_loss: 0.5607 - val_accuracy: 0.6824 - val_f1: 0.6863\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5349 - accuracy: 0.6991 - f1: 0.6865 - val_loss: 0.5647 - val_accuracy: 0.6797 - val_f1: 0.6642\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5340 - accuracy: 0.6948 - f1: 0.6803 - val_loss: 0.5730 - val_accuracy: 0.6742 - val_f1: 0.6955\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5322 - accuracy: 0.6967 - f1: 0.6999 - val_loss: 0.5598 - val_accuracy: 0.6895 - val_f1: 0.6889\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.5300 - accuracy: 0.6979 - f1: 0.6797 - val_loss: 0.5664 - val_accuracy: 0.6859 - val_f1: 0.6726\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5306 - accuracy: 0.6982 - f1: 0.6876 - val_loss: 0.5657 - val_accuracy: 0.6777 - val_f1: 0.6974\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5314 - accuracy: 0.6964 - f1: 0.6947 - val_loss: 0.5714 - val_accuracy: 0.6828 - val_f1: 0.6781\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5291 - accuracy: 0.6951 - f1: 0.6877 - val_loss: 0.5647 - val_accuracy: 0.6793 - val_f1: 0.6996\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5290 - accuracy: 0.6991 - f1: 0.6864 - val_loss: 0.5646 - val_accuracy: 0.6777 - val_f1: 0.6920\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5271 - accuracy: 0.7065 - f1: 0.7071 - val_loss: 0.5712 - val_accuracy: 0.6840 - val_f1: 0.6791\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5241 - accuracy: 0.7017 - f1: 0.6947 - val_loss: 0.5693 - val_accuracy: 0.6711 - val_f1: 0.6929\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5246 - accuracy: 0.7049 - f1: 0.6985 - val_loss: 0.5701 - val_accuracy: 0.6820 - val_f1: 0.6925\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5261 - accuracy: 0.7031 - f1: 0.7000 - val_loss: 0.5634 - val_accuracy: 0.6863 - val_f1: 0.6857\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5243 - accuracy: 0.7034 - f1: 0.6983 - val_loss: 0.5621 - val_accuracy: 0.6832 - val_f1: 0.7051\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5217 - accuracy: 0.7052 - f1: 0.7015 - val_loss: 0.5675 - val_accuracy: 0.6805 - val_f1: 0.7015\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.5199 - accuracy: 0.7038 - f1: 0.7015 - val_loss: 0.5658 - val_accuracy: 0.6824 - val_f1: 0.6864\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5202 - accuracy: 0.7063 - f1: 0.6998 - val_loss: 0.5661 - val_accuracy: 0.6891 - val_f1: 0.6913\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5230 - accuracy: 0.7082 - f1: 0.6963 - val_loss: 0.5642 - val_accuracy: 0.6801 - val_f1: 0.6785\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5204 - accuracy: 0.7032 - f1: 0.6971 - val_loss: 0.5729 - val_accuracy: 0.6734 - val_f1: 0.6853\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5204 - accuracy: 0.7051 - f1: 0.7051 - val_loss: 0.5661 - val_accuracy: 0.6812 - val_f1: 0.6943\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5213 - accuracy: 0.7071 - f1: 0.7049 - val_loss: 0.5755 - val_accuracy: 0.6777 - val_f1: 0.6818\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.5194 - accuracy: 0.7105 - f1: 0.7033 - val_loss: 0.5670 - val_accuracy: 0.6820 - val_f1: 0.6916\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5178 - accuracy: 0.7079 - f1: 0.7003 - val_loss: 0.5711 - val_accuracy: 0.6824 - val_f1: 0.6812\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5195 - accuracy: 0.7066 - f1: 0.7046 - val_loss: 0.5700 - val_accuracy: 0.6797 - val_f1: 0.6983\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.5196 - accuracy: 0.7026 - f1: 0.6994 - val_loss: 0.5656 - val_accuracy: 0.6816 - val_f1: 0.6787\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5205 - accuracy: 0.7079 - f1: 0.6965 - val_loss: 0.5703 - val_accuracy: 0.6781 - val_f1: 0.6959\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5171 - accuracy: 0.7132 - f1: 0.7173 - val_loss: 0.5745 - val_accuracy: 0.6805 - val_f1: 0.6795\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5161 - accuracy: 0.7099 - f1: 0.7073 - val_loss: 0.5704 - val_accuracy: 0.6852 - val_f1: 0.6998\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5170 - accuracy: 0.7077 - f1: 0.6997 - val_loss: 0.5716 - val_accuracy: 0.6816 - val_f1: 0.6974\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 0.5170 - accuracy: 0.7104 - f1: 0.7088 - val_loss: 0.5725 - val_accuracy: 0.6785 - val_f1: 0.6733\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.5164 - accuracy: 0.7108 - f1: 0.7079 - val_loss: 0.5715 - val_accuracy: 0.6816 - val_f1: 0.6738\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.5150 - accuracy: 0.7080 - f1: 0.7037 - val_loss: 0.5742 - val_accuracy: 0.6797 - val_f1: 0.6975\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 1s 205ms/step - loss: 0.5142 - accuracy: 0.7107 - f1: 0.7112 - val_loss: 0.5748 - val_accuracy: 0.6887 - val_f1: 0.6897\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.5167 - accuracy: 0.7053 - f1: 0.7030 - val_loss: 0.5661 - val_accuracy: 0.6777 - val_f1: 0.6961\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.5131 - accuracy: 0.7122 - f1: 0.7048 - val_loss: 0.5797 - val_accuracy: 0.6844 - val_f1: 0.7027\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.5136 - accuracy: 0.7119 - f1: 0.7221 - val_loss: 0.5775 - val_accuracy: 0.6871 - val_f1: 0.6901\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5114 - accuracy: 0.7088 - f1: 0.6958 - val_loss: 0.5748 - val_accuracy: 0.6617 - val_f1: 0.7020\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5110 - accuracy: 0.7090 - f1: 0.7116 - val_loss: 0.5817 - val_accuracy: 0.6832 - val_f1: 0.6990\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5110 - accuracy: 0.7153 - f1: 0.7209 - val_loss: 0.5808 - val_accuracy: 0.6840 - val_f1: 0.6944\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5145 - accuracy: 0.7139 - f1: 0.6912 - val_loss: 0.5763 - val_accuracy: 0.6762 - val_f1: 0.7011\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5141 - accuracy: 0.7105 - f1: 0.7201 - val_loss: 0.5829 - val_accuracy: 0.6824 - val_f1: 0.6854\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5090 - accuracy: 0.7173 - f1: 0.7137 - val_loss: 0.5739 - val_accuracy: 0.6801 - val_f1: 0.6999\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.5105 - accuracy: 0.7162 - f1: 0.7125 - val_loss: 0.5824 - val_accuracy: 0.6781 - val_f1: 0.6912\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 0.5083 - accuracy: 0.7205 - f1: 0.7284 - val_loss: 0.5756 - val_accuracy: 0.6906 - val_f1: 0.7002\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5125 - accuracy: 0.7134 - f1: 0.6996 - val_loss: 0.5770 - val_accuracy: 0.6844 - val_f1: 0.7025\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5077 - accuracy: 0.7172 - f1: 0.7252 - val_loss: 0.5772 - val_accuracy: 0.6844 - val_f1: 0.6978\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5080 - accuracy: 0.7152 - f1: 0.7074 - val_loss: 0.5734 - val_accuracy: 0.6793 - val_f1: 0.6871\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.5070 - accuracy: 0.7167 - f1: 0.7137 - val_loss: 0.5927 - val_accuracy: 0.6715 - val_f1: 0.6968\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.5066 - accuracy: 0.7150 - f1: 0.7151 - val_loss: 0.5801 - val_accuracy: 0.6820 - val_f1: 0.7003\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5083 - accuracy: 0.7157 - f1: 0.7218 - val_loss: 0.5821 - val_accuracy: 0.6801 - val_f1: 0.7004\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.5080 - accuracy: 0.7216 - f1: 0.7173 - val_loss: 0.5804 - val_accuracy: 0.6773 - val_f1: 0.6583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ae00018eb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the weights of the classes since your dataset is HIGHLY IMBALANCED!\n",
    "class_weight = {0: 1.,\n",
    "                1: 1.}\n",
    "\n",
    "model.fit(x_train.values,\n",
    "         y_train.values,\n",
    "         epochs = 300,\n",
    "         batch_size = 2048,\n",
    "         validation_data = (x_val.values, y_val.values), class_weight=class_weight,\n",
    "         callbacks=[EarlyStopping(monitor='val_f1', mode='max', patience=100, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the model is: 68.3125 %\n",
      "\n",
      "[[1014  570]\n",
      " [ 444 1172]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Fully Paid       0.70      0.64      0.67      1584\n",
      " Charged Off       0.67      0.73      0.70      1616\n",
      "\n",
      "    accuracy                           0.68      3200\n",
      "   macro avg       0.68      0.68      0.68      3200\n",
      "weighted avg       0.68      0.68      0.68      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model.predict(x_test.values)\n",
    "y_prediction= [1 if i>=0.5 else 0 for i in y_prediction]\n",
    "print(\"The Test Accuracy of the model is: {} %\".format(accuracy_score(y_test.values, y_prediction) * 100.)) \n",
    "print()\n",
    "\n",
    "print(confusion_matrix(y_test.values, y_prediction))\n",
    "print()\n",
    "\n",
    "target_names = ['Fully Paid', 'Charged Off']\n",
    "print(classification_report(y_test, y_prediction, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c29f24f490fc11ce1e48d89a3d8a10c86c424066fbebe55fdecaa30f8bedbaf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
